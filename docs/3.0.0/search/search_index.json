{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Iguana documentation! This documentation is currently under construction to adjust the needs of version 3.0.0 During this phase pages will be removed and changed so they won't work with any version prior 3.x However the documentation is archived for version 2 at documentation 2.x archive This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into two main points Usage Development In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test Usage Getting Started - What is Iguana and How to download and execute a benchmark using it. Architecture - Provides a basic understanding how Iguana is working. Configuration - How to configure Iguana. Implemented Tests - What test cases are implemented. Implemented Workers - What HTTP and CLI endpoints are testable. Tutorial - A complete tutorial using triple stores. Result File - How to read the result file. Development Architecture - What is the internal workflow of Iguana Extend Iguana - What and how to extend. Metrics - How to add your own result metric QueryHandler - Does your queries need a certain instantiation? LanguageProcessor - Tells Iguana how to read HTTP responses and analyze queries. Tasks - Add your own Benchmark test. If the stresstest isn't sufficient. Worker - HTTP Get and Post won't work. Your CLI application does things differently? Add your own worker then. Have exciting Evaluations!","title":"Home"},{"location":"#welcome-to-the-iguana-documentation","text":"This documentation is currently under construction to adjust the needs of version 3.0.0 During this phase pages will be removed and changed so they won't work with any version prior 3.x However the documentation is archived for version 2 at documentation 2.x archive This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into two main points Usage Development In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test","title":"Welcome to the Iguana documentation!"},{"location":"#usage","text":"Getting Started - What is Iguana and How to download and execute a benchmark using it. Architecture - Provides a basic understanding how Iguana is working. Configuration - How to configure Iguana. Implemented Tests - What test cases are implemented. Implemented Workers - What HTTP and CLI endpoints are testable. Tutorial - A complete tutorial using triple stores. Result File - How to read the result file.","title":"Usage"},{"location":"#development","text":"Architecture - What is the internal workflow of Iguana Extend Iguana - What and how to extend. Metrics - How to add your own result metric QueryHandler - Does your queries need a certain instantiation? LanguageProcessor - Tells Iguana how to read HTTP responses and analyze queries. Tasks - Add your own Benchmark test. If the stresstest isn't sufficient. Worker - HTTP Get and Post won't work. Your CLI application does things differently? Add your own worker then. Have exciting Evaluations!","title":"Development"},{"location":"about/","text":"Iguana Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"About"},{"location":"about/#iguana","text":"Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable","title":"Iguana"},{"location":"about/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"about/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"about/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"architecture/","text":"Architecture Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11. Overview Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application. Components Iguana consists of two components, the core controller and the result processor. core controller The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results. result processor The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt More Information SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"Architecture"},{"location":"architecture/#architecture","text":"Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11.","title":"Architecture"},{"location":"architecture/#overview","text":"Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application.","title":"Overview"},{"location":"architecture/#components","text":"Iguana consists of two components, the core controller and the result processor.","title":"Components"},{"location":"architecture/#core-controller","text":"The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results.","title":"core controller"},{"location":"architecture/#result-processor","text":"The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt","title":"result processor"},{"location":"architecture/#more-information","text":"SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"More Information"},{"location":"download/","text":"Download Prerequisites You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"download/#download","text":"","title":"Download"},{"location":"download/#prerequisites","text":"You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java","title":"Prerequisites"},{"location":"download/#download_1","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"quick-config/","text":"","title":"Quick Configuration"},{"location":"run-iguana/","text":"Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Run Iguana"},{"location":"run-iguana/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Start a Benchmark"},{"location":"shorthand-mapping/","text":"Shorthand Class Name Stresstest org.aksw.iguana.cc.tasks.impl.Stresstest ---------- ------- InstancesQueryHandler org.aksw.iguana.cc.query.impl.InstancesQueryHandler PatternQueryHandler org.aksw.iguana.cc.query.impl.PatternQueryHandler ---------- ------- lang.SPARQL org.aksw.iguana.cc.lang.impl.RDFLanguageProcessor lang.RDF org.aksw.iguana.cc.lang.impl.SPARQLLanguageProcessor ---------- ------- SPARQLWorker org.aksw.iguana.cc.worker.impl.SPARQLWorker UPDATEWorker org.aksw.iguana.cc.worker.impl.UPDATEWorker HttpPostWorker org.aksw.iguana.cc.worker.impl.HttpPostWorker HttpGetWorker org.aksw.iguana.cc.worker.impl.HttpGetWorker CLIWorker org.aksw.iguana.cc.worker.impl.CLIWorker CLIInputWorker org.aksw.iguana.cc.worker.impl.CLIInputWorker CLIInputFileWorker org.aksw.iguana.cc.worker.impl.CLIInputFileWorker CLIInputPrefixWorker org.aksw.iguana.cc.worker.impl.CLIInputPrefixWorker MultipleCLIInputWorker org.aksw.iguana.cc.worker.impl.MultipleCLIInputWorker ---------- ------- NTFileStorage org.aksw.iguana.rp.storages.impl.NTFileStorage TriplestoreStorage org.aksw.iguana.rp.storages.impl.TriplestoreStorage ---------- ------- QPS org.aksw.iguana.rp.metrics.impl.QPSMetric AvgQPS org.aksw.iguana.rp.metrics.impl.AvgQPSMetric NoQ org.aksw.iguana.rp.metrics.impl.NoQMetric NoQPH org.aksw.iguana.rp.metrics.impl.NoQPHMetric QMPH org.aksw.iguana.rp.metrics.impl.QMPHMetric EachQuery org.aksw.iguana.rp.metrics.impl.EQEMetric","title":"Shorthand mapping"},{"location":"develop/architecture/","text":"Test1 Test2","title":"Architecture"},{"location":"develop/architecture/#test1","text":"","title":"Test1"},{"location":"develop/architecture/#test2","text":"","title":"Test2"},{"location":"develop/extend-lang/","text":"","title":"Languages"},{"location":"develop/extend-metrics/","text":"","title":"Metrics"},{"location":"develop/extend-queryhandling/","text":"","title":"Query Handling"},{"location":"develop/extend-result-storages/","text":"","title":"Result storage"},{"location":"develop/extend-task/","text":"Extend Tasks Test1 Test2","title":"Tasks"},{"location":"develop/extend-task/#extend-tasks","text":"","title":"Extend Tasks"},{"location":"develop/extend-task/#test1","text":"","title":"Test1"},{"location":"develop/extend-task/#test2","text":"","title":"Test2"},{"location":"develop/extend-workers/","text":"","title":"Workers"},{"location":"develop/how-to-start/","text":"","title":"How to start"},{"location":"develop/maven/","text":"Use Iguana as a Maven dependency Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Maven"},{"location":"develop/maven/#use-iguana-as-a-maven-dependency","text":"Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Use Iguana as a Maven dependency"},{"location":"develop/overview/","text":"","title":"Overview"},{"location":"usage/configuration/","text":"Configuration The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana. Connections Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too. Datasets Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" Tasks A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: YourTask configuration: parameter1: value1 parameter2: value2 Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration. Storages Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here Metrics Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results . Task script hooks To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" Full Example connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\" Shorthand A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Configuration"},{"location":"usage/configuration/#configuration","text":"The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana.","title":"Configuration"},{"location":"usage/configuration/#connections","text":"Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too.","title":"Connections"},{"location":"usage/configuration/#datasets","text":"Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\"","title":"Datasets"},{"location":"usage/configuration/#tasks","text":"A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: YourTask configuration: parameter1: value1 parameter2: value2 Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration.","title":"Tasks"},{"location":"usage/configuration/#storages","text":"Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here","title":"Storages"},{"location":"usage/configuration/#metrics","text":"Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results .","title":"Metrics"},{"location":"usage/configuration/#task-script-hooks","text":"To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\"","title":"Task script hooks"},{"location":"usage/configuration/#full-example","text":"connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\"","title":"Full Example"},{"location":"usage/configuration/#shorthand","text":"A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Shorthand"},{"location":"usage/getting-started/","text":"What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint. Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki. Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Getting started"},{"location":"usage/getting-started/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"usage/getting-started/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"usage/getting-started/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"usage/getting-started/#download","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"usage/getting-started/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Start a Benchmark"},{"location":"usage/languages/","text":"","title":"Supported Languages"},{"location":"usage/metrics-and-store/","text":"","title":"Metrics & Store"},{"location":"usage/queries/","text":"","title":"Supported Queries"},{"location":"usage/results/","text":"","title":"Benchmark Results"},{"location":"usage/script-hooks/","text":"","title":"Script Hooks"},{"location":"usage/stresstest/","text":"","title":"Stresstest"},{"location":"usage/tutorial/","text":"Tutorial","title":"Tutorial"},{"location":"usage/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"usage/workers/","text":"","title":"Supported Workers"},{"location":"usage/workflow/","text":"","title":"Workflow"}]}