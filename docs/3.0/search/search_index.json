{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Iguana documentation! This documentation is currently under construction to adjust the needs of version 3.0.0 During this phase pages will be removed and changed so they won't work with any version prior 3.x However the documentation is archived for version 2 at documentation 2.x archive This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into three parts General Quick Start Guide Usage Development In General you will find a bit of information of what Iguana is and what it's capable of. In the Quick Start Guide you will find how to download and start Iguana as well how to quickly configure your first simple benchmark using Iguana. In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test Have exciting Evaluations!","title":"Home"},{"location":"#welcome-to-the-iguana-documentation","text":"This documentation is currently under construction to adjust the needs of version 3.0.0 During this phase pages will be removed and changed so they won't work with any version prior 3.x However the documentation is archived for version 2 at documentation 2.x archive This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into three parts General Quick Start Guide Usage Development In General you will find a bit of information of what Iguana is and what it's capable of. In the Quick Start Guide you will find how to download and start Iguana as well how to quickly configure your first simple benchmark using Iguana. In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test Have exciting Evaluations!","title":"Welcome to the Iguana documentation!"},{"location":"about/","text":"Iguana Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"About"},{"location":"about/#iguana","text":"Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable","title":"Iguana"},{"location":"about/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"about/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"about/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"architecture/","text":"Architecture Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11. Overview Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application. Components Iguana consists of two components, the core controller and the result processor. core controller The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results. result processor The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt More Information SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"Architecture"},{"location":"architecture/#architecture","text":"Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11.","title":"Architecture"},{"location":"architecture/#overview","text":"Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application.","title":"Overview"},{"location":"architecture/#components","text":"Iguana consists of two components, the core controller and the result processor.","title":"Components"},{"location":"architecture/#core-controller","text":"The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results.","title":"core controller"},{"location":"architecture/#result-processor","text":"The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt","title":"result processor"},{"location":"architecture/#more-information","text":"SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"More Information"},{"location":"download/","text":"Download Prerequisites You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana.corecontroller-3.0.2.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"download/#download","text":"","title":"Download"},{"location":"download/#prerequisites","text":"You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java","title":"Prerequisites"},{"location":"download/#download_1","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana.corecontroller-3.0.2.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"quick-config/","text":"Quickly Configure Iguana Here we will setup a quick configuration which will benchmark one triple store (e.g. apache jena fuseki) using one simulated user. We assume that your triple store (or whatever HTTP GET endpoint you want to use) is running and loaded with data. For now we assume that the endpoint is at http://localhost:3030/ds/sparql and uses GET with the parameter query Further on the benchmark should take 10 minutes (or 60.000 ms) and uses plain text queries located in queries.txt . If you do not have created some queries yet, use these for example SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} and save them to queries.txt . Your results will be written as an N-Triple file to first-benchmark-results.nt The following configuration works with these demands. # you can ignore this for now datasets: - name: \"Dataset\" #Your connection connections: - name: \"Fuseki\" # Change this to your actual endpoint you want to use endpoint: \"http://localhost:3030/ds/sparql\" # The benchmark task tasks: - className: \"Stresstest\" configuration: # 10 minutes (time Limit is in ms) timeLimit: 600000 # we are using plain text queries queryHandler: className: \"InstancesQueryHandler\" # create one SPARQL Worker (it's basically a HTTP get worker using the 'query' parameter # it uses the queries.txt file as benchmark queries workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" # tell Iguana where to save your results to storages: - className: \"NTFileStorage\" configuration: fileName: \"first-benchmark-results.nt\" For more information on the confguration have a look at Configuration","title":"Quick Configuration"},{"location":"quick-config/#quickly-configure-iguana","text":"Here we will setup a quick configuration which will benchmark one triple store (e.g. apache jena fuseki) using one simulated user. We assume that your triple store (or whatever HTTP GET endpoint you want to use) is running and loaded with data. For now we assume that the endpoint is at http://localhost:3030/ds/sparql and uses GET with the parameter query Further on the benchmark should take 10 minutes (or 60.000 ms) and uses plain text queries located in queries.txt . If you do not have created some queries yet, use these for example SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} and save them to queries.txt . Your results will be written as an N-Triple file to first-benchmark-results.nt The following configuration works with these demands. # you can ignore this for now datasets: - name: \"Dataset\" #Your connection connections: - name: \"Fuseki\" # Change this to your actual endpoint you want to use endpoint: \"http://localhost:3030/ds/sparql\" # The benchmark task tasks: - className: \"Stresstest\" configuration: # 10 minutes (time Limit is in ms) timeLimit: 600000 # we are using plain text queries queryHandler: className: \"InstancesQueryHandler\" # create one SPARQL Worker (it's basically a HTTP get worker using the 'query' parameter # it uses the queries.txt file as benchmark queries workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" # tell Iguana where to save your results to storages: - className: \"NTFileStorage\" configuration: fileName: \"first-benchmark-results.nt\" For more information on the confguration have a look at Configuration","title":"Quickly Configure Iguana"},{"location":"run-iguana/","text":"Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml To set JVM options, you can use $IGUANA_JVM For example to let Iguana use 4GB of RAM you can set the IGUANA_JVM as follows export IGUANA_JVM=-Xmx4g and start as above. or using the jar with java 11 as follows java -jar iguana-corecontroller-3.0.2.jar example-suite.yml","title":"Run Iguana"},{"location":"run-iguana/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml To set JVM options, you can use $IGUANA_JVM For example to let Iguana use 4GB of RAM you can set the IGUANA_JVM as follows export IGUANA_JVM=-Xmx4g and start as above. or using the jar with java 11 as follows java -jar iguana-corecontroller-3.0.2.jar example-suite.yml","title":"Start a Benchmark"},{"location":"shorthand-mapping/","text":"Shorthand Class Name Stresstest org.aksw.iguana.cc.tasks.impl.Stresstest ---------- ------- InstancesQueryHandler org.aksw.iguana.cc.query.impl.InstancesQueryHandler PatternQueryHandler org.aksw.iguana.cc.query.impl.PatternQueryHandler ---------- ------- lang.RDF org.aksw.iguana.cc.lang.impl.RDFLanguageProcessor lang.SPARQL org.aksw.iguana.cc.lang.impl.SPARQLLanguageProcessor ---------- ------- SPARQLWorker org.aksw.iguana.cc.worker.impl.SPARQLWorker UPDATEWorker org.aksw.iguana.cc.worker.impl.UPDATEWorker HttpPostWorker org.aksw.iguana.cc.worker.impl.HttpPostWorker HttpGetWorker org.aksw.iguana.cc.worker.impl.HttpGetWorker CLIWorker org.aksw.iguana.cc.worker.impl.CLIWorker CLIInputWorker org.aksw.iguana.cc.worker.impl.CLIInputWorker CLIInputFileWorker org.aksw.iguana.cc.worker.impl.CLIInputFileWorker CLIInputPrefixWorker org.aksw.iguana.cc.worker.impl.CLIInputPrefixWorker MultipleCLIInputWorker org.aksw.iguana.cc.worker.impl.MultipleCLIInputWorker ---------- ------- NTFileStorage org.aksw.iguana.rp.storages.impl.NTFileStorage TriplestoreStorage org.aksw.iguana.rp.storages.impl.TriplestoreStorage ---------- ------- QPS org.aksw.iguana.rp.metrics.impl.QPSMetric AvgQPS org.aksw.iguana.rp.metrics.impl.AvgQPSMetric NoQ org.aksw.iguana.rp.metrics.impl.NoQMetric NoQPH org.aksw.iguana.rp.metrics.impl.NoQPHMetric QMPH org.aksw.iguana.rp.metrics.impl.QMPHMetric EachQuery org.aksw.iguana.rp.metrics.impl.EQEMetric","title":"Shorthand mapping"},{"location":"develop/architecture/","text":"Test1 Test2","title":"Architecture"},{"location":"develop/architecture/#test1","text":"","title":"Test1"},{"location":"develop/architecture/#test2","text":"","title":"Test2"},{"location":"develop/extend-lang/","text":"","title":"Languages"},{"location":"develop/extend-metrics/","text":"","title":"Metrics"},{"location":"develop/extend-queryhandling/","text":"","title":"Query Handling"},{"location":"develop/extend-result-storages/","text":"","title":"Result storage"},{"location":"develop/extend-task/","text":"Extend Tasks Test1 Test2","title":"Tasks"},{"location":"develop/extend-task/#extend-tasks","text":"","title":"Extend Tasks"},{"location":"develop/extend-task/#test1","text":"","title":"Test1"},{"location":"develop/extend-task/#test2","text":"","title":"Test2"},{"location":"develop/extend-workers/","text":"","title":"Workers"},{"location":"develop/how-to-start/","text":"","title":"How to start"},{"location":"develop/maven/","text":"Use Iguana as a Maven dependency Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Maven"},{"location":"develop/maven/#use-iguana-as-a-maven-dependency","text":"Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Use Iguana as a Maven dependency"},{"location":"develop/overview/","text":"","title":"Overview"},{"location":"usage/configuration/","text":"Configuration The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana. Connections Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too. Datasets Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" Tasks A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: \"YourTask\" configuration: parameter1: value1 parameter2: \"value2\" Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration. Storages Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here Metrics Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results . Task script hooks To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" Full Example connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\" Shorthand A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Configuration"},{"location":"usage/configuration/#configuration","text":"The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana.","title":"Configuration"},{"location":"usage/configuration/#connections","text":"Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too.","title":"Connections"},{"location":"usage/configuration/#datasets","text":"Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\"","title":"Datasets"},{"location":"usage/configuration/#tasks","text":"A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: \"YourTask\" configuration: parameter1: value1 parameter2: \"value2\" Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration.","title":"Tasks"},{"location":"usage/configuration/#storages","text":"Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here","title":"Storages"},{"location":"usage/configuration/#metrics","text":"Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results .","title":"Metrics"},{"location":"usage/configuration/#task-script-hooks","text":"To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\"","title":"Task script hooks"},{"location":"usage/configuration/#full-example","text":"connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\"","title":"Full Example"},{"location":"usage/configuration/#shorthand","text":"A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Shorthand"},{"location":"usage/getting-started/","text":"What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint. Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki. Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Getting started"},{"location":"usage/getting-started/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"usage/getting-started/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"usage/getting-started/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"usage/getting-started/#download","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/tag/v3.0.0 The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"usage/getting-started/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.0.0.jar example-suite.yml","title":"Start a Benchmark"},{"location":"usage/languages/","text":"Supported Languages The Language tag is set to assure that the result size returned by the benchmarked system is correctly read and that result can give a little extra query statistics. Currently two languages are implemented, however you can use lang.SPARQL or simply ignore it all the way. If they are not in SPARQL the query statistics will be just containing the query text and the result size will be read as if each returned line were one result. The two languages are: lang.SPARQL lang.RDF","title":"Supported Languages"},{"location":"usage/languages/#supported-languages","text":"The Language tag is set to assure that the result size returned by the benchmarked system is correctly read and that result can give a little extra query statistics. Currently two languages are implemented, however you can use lang.SPARQL or simply ignore it all the way. If they are not in SPARQL the query statistics will be just containing the query text and the result size will be read as if each returned line were one result. The two languages are: lang.SPARQL lang.RDF","title":"Supported Languages"},{"location":"usage/metrics/","text":"Implemented Metrics Every metric will be calculated globally (for one Experiment Task) and locally (for each Worker) Hence you can just analyze the overall metrics or if you want to look closer, you can look at each worker. NoQ The number of successfully executed Queries QMPH The number of executed Query Mixes Per Hour NoQPH The number of successfully executed Number of Queries Per Hour QPS For each query the queries per second , the total time in ms (summed up time of each execution), the no of succeeded and failed executions and the result size will be saved. Additionaly will try to tell how many times a query failed with what reason. ( timeout , wrong return code e.g. 400, or unknown ) Further on the QPS metrics provides a penalized QPS which penalizes queries which will fail. As some systems who cannot resolve a query just returns an error code and thus can have a very high score, even though they could only handle a few queries it would be rather unfair to the compared systems. Thus we introduced the penalty QPS. It is calculated the same as the QPS score, but for each failed query it uses the penalty instead of the actual time the failed query took. The default is set to the timeOut of the task. However you can override it as follows: metrics: - className: \"QPS\" configuration: #in MS penality: 10000 AvgQPS The average of all queries per second. EachQuery Will save every query execution. (Experimental)","title":"Metrics"},{"location":"usage/metrics/#implemented-metrics","text":"Every metric will be calculated globally (for one Experiment Task) and locally (for each Worker) Hence you can just analyze the overall metrics or if you want to look closer, you can look at each worker.","title":"Implemented Metrics"},{"location":"usage/metrics/#noq","text":"The number of successfully executed Queries","title":"NoQ"},{"location":"usage/metrics/#qmph","text":"The number of executed Query Mixes Per Hour","title":"QMPH"},{"location":"usage/metrics/#noqph","text":"The number of successfully executed Number of Queries Per Hour","title":"NoQPH"},{"location":"usage/metrics/#qps","text":"For each query the queries per second , the total time in ms (summed up time of each execution), the no of succeeded and failed executions and the result size will be saved. Additionaly will try to tell how many times a query failed with what reason. ( timeout , wrong return code e.g. 400, or unknown ) Further on the QPS metrics provides a penalized QPS which penalizes queries which will fail. As some systems who cannot resolve a query just returns an error code and thus can have a very high score, even though they could only handle a few queries it would be rather unfair to the compared systems. Thus we introduced the penalty QPS. It is calculated the same as the QPS score, but for each failed query it uses the penalty instead of the actual time the failed query took. The default is set to the timeOut of the task. However you can override it as follows: metrics: - className: \"QPS\" configuration: #in MS penality: 10000","title":"QPS"},{"location":"usage/metrics/#avgqps","text":"The average of all queries per second.","title":"AvgQPS"},{"location":"usage/metrics/#eachquery","text":"Will save every query execution. (Experimental)","title":"EachQuery"},{"location":"usage/queries/","text":"Supported Queries There are currently two query types supported: plain text queries SPARQL pattern queries Plain Text Queries This can be anything: SPARQL, SQL, a whole book if you need to. The only limitation is that it has to fit in one line per query. Every query can be executed as is. This can be set using the following: ... queryHandler: className: \"InstancesQueryHandler\" SPARQL Pattern Queries This only works for SPARQL Queries at the moment. The idea came from the DBpedia SPARQL Benchmark paper from 2011 and 2012. Instead of SPARQL queries as they are, you can set variables, which will be exchanged with real data. Hence Iguana can create thousands of queries using a SPARQL pattern query. A pattern query might look like the following: SELECT * {?s rdf:type %%var0%% ; %%var1%% %%var2%%. %%var2%% ?p ?o} This query in itself cannot be send to a triple store, however we can exchange the variables using real data. Thus we need a reference endpoint (ideally) containing the same data as the dataset which will be tested. This query will then be exchanged to SELECT ?var0 ?var1 ?var2 {?s rdf:type ?var0 ; ?var1 ?var2. ?var2 ?p ?o} LIMIT 2000 and be queried against the reference endpoint. For each result (limited to 2000) a query instance will be created. This will be done for every query in the benchmark queries. All instances of these query patterns will be subsummed as if they were one query in the results. This can be set using the following: ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" or ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" limit: 4000","title":"Supported Queries"},{"location":"usage/queries/#supported-queries","text":"There are currently two query types supported: plain text queries SPARQL pattern queries","title":"Supported Queries"},{"location":"usage/queries/#plain-text-queries","text":"This can be anything: SPARQL, SQL, a whole book if you need to. The only limitation is that it has to fit in one line per query. Every query can be executed as is. This can be set using the following: ... queryHandler: className: \"InstancesQueryHandler\"","title":"Plain Text Queries"},{"location":"usage/queries/#sparql-pattern-queries","text":"This only works for SPARQL Queries at the moment. The idea came from the DBpedia SPARQL Benchmark paper from 2011 and 2012. Instead of SPARQL queries as they are, you can set variables, which will be exchanged with real data. Hence Iguana can create thousands of queries using a SPARQL pattern query. A pattern query might look like the following: SELECT * {?s rdf:type %%var0%% ; %%var1%% %%var2%%. %%var2%% ?p ?o} This query in itself cannot be send to a triple store, however we can exchange the variables using real data. Thus we need a reference endpoint (ideally) containing the same data as the dataset which will be tested. This query will then be exchanged to SELECT ?var0 ?var1 ?var2 {?s rdf:type ?var0 ; ?var1 ?var2. ?var2 ?p ?o} LIMIT 2000 and be queried against the reference endpoint. For each result (limited to 2000) a query instance will be created. This will be done for every query in the benchmark queries. All instances of these query patterns will be subsummed as if they were one query in the results. This can be set using the following: ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" or ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" limit: 4000","title":"SPARQL Pattern Queries"},{"location":"usage/results/","text":"Experiment Results Fundamentals The results are saved into RDF. For those who don't know what RDF is, it is best described as a way to represent a directed graph. The according query language is called SPARQL. The graph schema of an iguana result is shown above, where as each node represents a class object containg several annotations. To retrieve all TaskIDs you can do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . } Let's look at an example to clarify how to request the global NoQ metric for a taskID you already know. Let's assume the taskID is 123/1/1 PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?noq { ires:123/1/1 iprop:NoQ ?noq } If you want to get all the local worker NoQ metrics do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?workerID ?noq { ires:123/1/1 iprop:workerResult ?workerID ?workerID iprop:NoQ ?noq } However if you just want to see the global NoQ metric for all taskIDs in your results do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . ?taskID iprop:NoQ ?noq. } Instead of the NoQ metric you can do this for all other metrics, except QPS . To retrieve QPS look above in the results schema and let's look at an example. Let's assume the taskID is 123/1/1 again. You can retrieve the global qps values (seen above in ExecutedQueries, e.g QPS , succeeded etc.) as follows, PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?failed ?resultSize { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:failed ?failed . ?executedQuery iprop:resultSize ?resultSize . } This will get you the QPS value, the no. of failed queries and the result size of the query. SPARQL Query statistics If you were using SPARQL queries as your benchmark queries you can add addtional further statistics of a query, such as: does the query has a FILTER. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?hasFilter ?queryText { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:queryID ?query . ?query iprop:filter ?hasFilter . ?query rdfs:label ?queryText . } This provides the qps value, if the SPARQL query has a filter and the actual query string. Ontology The results ontology (description of what each property and class means) can be found here Adding LSQ analyzation If you're using SPARQL and want some more indepth analysation of the query statistics, you can use LSQ to do so. Iguana will add an owl:sameAs link between the SPARQL queries used in your benchmark and the equivalent LSQ query links. Hence you can run the performance measurement using Iguana and the query analyzation using LSQ independently and combine both results afterwards","title":"Benchmark Results"},{"location":"usage/results/#experiment-results","text":"","title":"Experiment Results"},{"location":"usage/results/#fundamentals","text":"The results are saved into RDF. For those who don't know what RDF is, it is best described as a way to represent a directed graph. The according query language is called SPARQL. The graph schema of an iguana result is shown above, where as each node represents a class object containg several annotations. To retrieve all TaskIDs you can do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . } Let's look at an example to clarify how to request the global NoQ metric for a taskID you already know. Let's assume the taskID is 123/1/1 PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?noq { ires:123/1/1 iprop:NoQ ?noq } If you want to get all the local worker NoQ metrics do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?workerID ?noq { ires:123/1/1 iprop:workerResult ?workerID ?workerID iprop:NoQ ?noq } However if you just want to see the global NoQ metric for all taskIDs in your results do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . ?taskID iprop:NoQ ?noq. } Instead of the NoQ metric you can do this for all other metrics, except QPS . To retrieve QPS look above in the results schema and let's look at an example. Let's assume the taskID is 123/1/1 again. You can retrieve the global qps values (seen above in ExecutedQueries, e.g QPS , succeeded etc.) as follows, PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?failed ?resultSize { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:failed ?failed . ?executedQuery iprop:resultSize ?resultSize . } This will get you the QPS value, the no. of failed queries and the result size of the query.","title":"Fundamentals"},{"location":"usage/results/#sparql-query-statistics","text":"If you were using SPARQL queries as your benchmark queries you can add addtional further statistics of a query, such as: does the query has a FILTER. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?hasFilter ?queryText { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:queryID ?query . ?query iprop:filter ?hasFilter . ?query rdfs:label ?queryText . } This provides the qps value, if the SPARQL query has a filter and the actual query string.","title":"SPARQL Query statistics"},{"location":"usage/results/#ontology","text":"The results ontology (description of what each property and class means) can be found here","title":"Ontology"},{"location":"usage/results/#adding-lsq-analyzation","text":"If you're using SPARQL and want some more indepth analysation of the query statistics, you can use LSQ to do so. Iguana will add an owl:sameAs link between the SPARQL queries used in your benchmark and the equivalent LSQ query links. Hence you can run the performance measurement using Iguana and the query analyzation using LSQ independently and combine both results afterwards","title":"Adding LSQ analyzation"},{"location":"usage/stresstest/","text":"Stresstest Iguanas implemented Stresstest benchmark task tries to emulate a real case scenario under which an endpoint or application is under high stress. As in real life endpoints might get multiple simultaneous request within seconds, it is very important to verify that you application can handle this. The stresstest emulates users or applications which will bombard the endpoint using a set of queries for a specific amount of time or a specific amount of queries executed. Each simulated user is called Worker in the following. As you might want to test read and write performance or just want to emulate different user behaviour, the stresstest allows to configure several workers. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. Configuration To configure this task you have to first tell Iguana to use the implemented task like the following: tasks: - className: \"Stresstest\" Further on you need to configure the Stresstest using the configuration parameter like: tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 ... As an end restriction you can either use timeLimit which will stop the stresstest after the specified amount in ms or you can set noOfQueryMixes which stops every worker after they executed the amount of queries in the provided query set. Additionaly to either timeLimit or noOfQueryMixes you can set the following parameters queryHandler workers warmup (optional) Query Handling The queryHandler parameter let's the stresstest know what queries will be used. Normally you will need the InstancesQueryHandler which will use plain text queries (could be SQL, SPARQL, a whole RDF document). The only restriction is that each query has to be in one line. You can set the query handler like the following: tasks: - className: \"Stresstest\" queryHandler: className: \"InstancesQueryHandler\" ... To see which query handlers are supported see Supported Queries Workers (simulated Users) Further on you have to add which workers to use. As described above you can set different worker configurations. Let's look at an example: - className: \"Stresstest\" timeLimit: 600000 workers: - threads: 4 className: \"SPARQLWorker\" queriesFile: \"/path/to/your/queries.txt\" - threads: 16 className: \"SPARQLWorker\" queriesFile: \"/other/queries.txt\" fixedLatency: 5000 In this example we have two different worker configurations we want to use. The first want will create 4 SPARQLWorker s using queries at /path/to/your/queries.txt with any latencym thus every query will be executed immediatly after another. The second worker configuration will execute 16 SPARQLWorker s using queries at /other/queries.txt using a fixed waiting time of 5000ms between each query. Hence every worker will execute their queries independently from each other but will wait 5s after each of their query execution before executing the next one. This configuration may simulate that we have a few Users requesting your endpoint locally (e.g. some of your application relying on your database) and several users querying your endpoint from outside the network where we would have network latency and other interferences which we will try to simulate with 5s. A full list of supported workers and their parameters can be found at Supported Workers In this example our Stresstest would create 20 workers, which will simultaenously request the endpoint for 60000ms (10 minutes). Warmup Additionaly to these you can optionally set a warmup, which will aim to let the system be benchmarked under a normal situation (Some times a database is faster when it was already running for a bit) The configuration is similar to the stresstest itself you can set a timeLimit (however not a certain no of query executions), you can set different workers , and a queryHandler to use. If you don't set the queryHandler parameter the warmup will simply use the queryHandler specified in the Stresstest itself. You can set the Warmup as following: tasks: - className: \"Stresstest\" warmup: timeLimit: 600000 workers: ... queryHandler: ... That's it. A full example might look like this tasks: - className: \"Stresstest\" configuration: # 1 hour (time Limit is in ms) timeLimit: 3600000 # warmup is optional warmup: # 10 minutes (is in ms) timeLimit: 600000 # queryHandler could be set too, same as in the stresstest configuration, otherwise the same queryHandler will be use. # workers are set the same way as in the configuration part workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries_warmup.txt\" timeOut: 180000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 16 className: \"SPARQLWorker\" queriesFile: \"queries_easy.txt\" timeOut: 180000 - threads: 4 className: \"SPARQLWorker\" queriesFile: \"queries_complex.txt\" fixedLatency: 100 References Supported Queries Supported Workers","title":"Stresstest"},{"location":"usage/stresstest/#stresstest","text":"Iguanas implemented Stresstest benchmark task tries to emulate a real case scenario under which an endpoint or application is under high stress. As in real life endpoints might get multiple simultaneous request within seconds, it is very important to verify that you application can handle this. The stresstest emulates users or applications which will bombard the endpoint using a set of queries for a specific amount of time or a specific amount of queries executed. Each simulated user is called Worker in the following. As you might want to test read and write performance or just want to emulate different user behaviour, the stresstest allows to configure several workers. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query.","title":"Stresstest"},{"location":"usage/stresstest/#configuration","text":"To configure this task you have to first tell Iguana to use the implemented task like the following: tasks: - className: \"Stresstest\" Further on you need to configure the Stresstest using the configuration parameter like: tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 ... As an end restriction you can either use timeLimit which will stop the stresstest after the specified amount in ms or you can set noOfQueryMixes which stops every worker after they executed the amount of queries in the provided query set. Additionaly to either timeLimit or noOfQueryMixes you can set the following parameters queryHandler workers warmup (optional)","title":"Configuration"},{"location":"usage/stresstest/#query-handling","text":"The queryHandler parameter let's the stresstest know what queries will be used. Normally you will need the InstancesQueryHandler which will use plain text queries (could be SQL, SPARQL, a whole RDF document). The only restriction is that each query has to be in one line. You can set the query handler like the following: tasks: - className: \"Stresstest\" queryHandler: className: \"InstancesQueryHandler\" ... To see which query handlers are supported see Supported Queries","title":"Query Handling"},{"location":"usage/stresstest/#workers-simulated-users","text":"Further on you have to add which workers to use. As described above you can set different worker configurations. Let's look at an example: - className: \"Stresstest\" timeLimit: 600000 workers: - threads: 4 className: \"SPARQLWorker\" queriesFile: \"/path/to/your/queries.txt\" - threads: 16 className: \"SPARQLWorker\" queriesFile: \"/other/queries.txt\" fixedLatency: 5000 In this example we have two different worker configurations we want to use. The first want will create 4 SPARQLWorker s using queries at /path/to/your/queries.txt with any latencym thus every query will be executed immediatly after another. The second worker configuration will execute 16 SPARQLWorker s using queries at /other/queries.txt using a fixed waiting time of 5000ms between each query. Hence every worker will execute their queries independently from each other but will wait 5s after each of their query execution before executing the next one. This configuration may simulate that we have a few Users requesting your endpoint locally (e.g. some of your application relying on your database) and several users querying your endpoint from outside the network where we would have network latency and other interferences which we will try to simulate with 5s. A full list of supported workers and their parameters can be found at Supported Workers In this example our Stresstest would create 20 workers, which will simultaenously request the endpoint for 60000ms (10 minutes).","title":"Workers (simulated Users)"},{"location":"usage/stresstest/#warmup","text":"Additionaly to these you can optionally set a warmup, which will aim to let the system be benchmarked under a normal situation (Some times a database is faster when it was already running for a bit) The configuration is similar to the stresstest itself you can set a timeLimit (however not a certain no of query executions), you can set different workers , and a queryHandler to use. If you don't set the queryHandler parameter the warmup will simply use the queryHandler specified in the Stresstest itself. You can set the Warmup as following: tasks: - className: \"Stresstest\" warmup: timeLimit: 600000 workers: ... queryHandler: ... That's it. A full example might look like this tasks: - className: \"Stresstest\" configuration: # 1 hour (time Limit is in ms) timeLimit: 3600000 # warmup is optional warmup: # 10 minutes (is in ms) timeLimit: 600000 # queryHandler could be set too, same as in the stresstest configuration, otherwise the same queryHandler will be use. # workers are set the same way as in the configuration part workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries_warmup.txt\" timeOut: 180000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 16 className: \"SPARQLWorker\" queriesFile: \"queries_easy.txt\" timeOut: 180000 - threads: 4 className: \"SPARQLWorker\" queriesFile: \"queries_complex.txt\" fixedLatency: 100","title":"Warmup"},{"location":"usage/stresstest/#references","text":"Supported Queries Supported Workers","title":"References"},{"location":"usage/tutorial/","text":"Tutorial Download Setting Up Systems Creating Benchmark Configuration Starting Benchmark Results","title":"Tutorial"},{"location":"usage/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"usage/tutorial/#download","text":"","title":"Download"},{"location":"usage/tutorial/#setting-up-systems","text":"","title":"Setting Up Systems"},{"location":"usage/tutorial/#creating-benchmark-configuration","text":"","title":"Creating Benchmark Configuration"},{"location":"usage/tutorial/#starting-benchmark","text":"","title":"Starting Benchmark"},{"location":"usage/tutorial/#results","text":"","title":"Results"},{"location":"usage/workers/","text":"Supported Workers A Worker is basically just a thread querying the endpoint/application. It tries to emulate a single user/application requesting your system until it should stop. In a task (e.g. the stresstest ) you can configure several worker configurations which will then be used inside the task. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. There a few workers implemented, which can be seperated into two main categories Http Workers CLI Workers Http Workers These Workers can be used to benchmark Http Applications (such as a SPARQL endpoint). Http Get Worker A Http worker using GET requests. This worker will use the endpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpGetWorker\" timeOut: 180000 parameterName: \"text\" This will use one HttpGetWOrker using a timeout of 3 minutes and the get parameter text to request the query through. Http Post Worker A Http worker using POST requests. This worker will use the updateEndpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) contentType yes text/plain The content type of the update queries. Setting the Content-Type: header responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpPostWorker\" timeOut: 180000 This will use one HttpGetWOrker using a timeout of 3 minutes. SPARQL Worker Simply a GET worker but the language parameter is set to lang.SPARQL . Otherwise see the Http Get Worker for configuration An Example: ... workers: - threads: 1 className: \"SPARQLWorker\" timeOut: 180000 SPARQL UPDATE Worker Simply a POST worker but specified for SPARQL Updates. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timerStrategy yes NONE NONE , FIXED or DISTRIBUTED . see below for explanation. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. The timerStrategy parameter let's the worker know how to distribute the updates. The fixedLatency and gaussianLatency parameters are not affected, the worker will wait those additionally. NONE: the worker just updates each update query after another FIXED: calculating the distribution by timeLimit / #updates at the start and waiting the amount between each update. Time Limit will be used of the task the worker is executed in. DISTRIBUTED: calculating the time to wait between two updates after each update by timeRemaining / #updatesRemaining . An Example: ... workers: - threads: 1 className: \"UPDATEWorker\" timeOut: 180000 timerStrategy: \"FIXED\" CLI Workers These workers can be used to benchmark a CLI application. CLI Worker This Worker should be used if the CLI application runs a query once and exits afterwards. Something like $ cli-script.sh query HEADER QUERY RESULT 1 QUERY RESULT 2 ... $ Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIWorker\" CLI Input Worker This Worker should be used if the CLI application runs and the query will be send using the Input. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" Multiple CLI Input Worker This Worker should be used if the CLI application runs and the query will be send using the Input and will quit on errors. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: ERROR ERROR happend, exiting $ To assure a smooth benchmark, the CLI application will be run multiple times instead of once, and if the application quits, the next running process will be used, while in the background the old process will be restarted. Thus as soon as an error happend, the benchmark can continue without a problem. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see above. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"MultipleCLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" CLI Input File Worker Same as the Multiple CLI Input Worker . However the query won't be send to the input but written to a file and the file will be send to the input Something like $ cli-script.sh start Your Input: file-containg-the-query.txt HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend directory no Directory in which the file including the query should be saved. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputFileWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" directory: \"/tmp/\" CLI Input Prefix Worker Same as the Multiple CLI Input Worker . However the CLI application might need a pre and suffix. Something like $ cli-script.sh start Your Input: PREFIX QUERY SUFFIX HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend queryPrefix no String to use as a PREFIX before the query. querySuffix no String to use as a SUFFIX after the query. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputPrefixWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" queryPrefix: \"SPARQL\" querySuffix: \";\" Will send the following as Input SPARQL QUERY ;","title":"Supported Workers"},{"location":"usage/workers/#supported-workers","text":"A Worker is basically just a thread querying the endpoint/application. It tries to emulate a single user/application requesting your system until it should stop. In a task (e.g. the stresstest ) you can configure several worker configurations which will then be used inside the task. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. There a few workers implemented, which can be seperated into two main categories Http Workers CLI Workers","title":"Supported Workers"},{"location":"usage/workers/#http-workers","text":"These Workers can be used to benchmark Http Applications (such as a SPARQL endpoint).","title":"Http Workers"},{"location":"usage/workers/#http-get-worker","text":"A Http worker using GET requests. This worker will use the endpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpGetWorker\" timeOut: 180000 parameterName: \"text\" This will use one HttpGetWOrker using a timeout of 3 minutes and the get parameter text to request the query through.","title":"Http Get Worker"},{"location":"usage/workers/#http-post-worker","text":"A Http worker using POST requests. This worker will use the updateEndpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) contentType yes text/plain The content type of the update queries. Setting the Content-Type: header responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpPostWorker\" timeOut: 180000 This will use one HttpGetWOrker using a timeout of 3 minutes.","title":"Http Post Worker"},{"location":"usage/workers/#sparql-worker","text":"Simply a GET worker but the language parameter is set to lang.SPARQL . Otherwise see the Http Get Worker for configuration An Example: ... workers: - threads: 1 className: \"SPARQLWorker\" timeOut: 180000","title":"SPARQL Worker"},{"location":"usage/workers/#sparql-update-worker","text":"Simply a POST worker but specified for SPARQL Updates. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timerStrategy yes NONE NONE , FIXED or DISTRIBUTED . see below for explanation. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. The timerStrategy parameter let's the worker know how to distribute the updates. The fixedLatency and gaussianLatency parameters are not affected, the worker will wait those additionally. NONE: the worker just updates each update query after another FIXED: calculating the distribution by timeLimit / #updates at the start and waiting the amount between each update. Time Limit will be used of the task the worker is executed in. DISTRIBUTED: calculating the time to wait between two updates after each update by timeRemaining / #updatesRemaining . An Example: ... workers: - threads: 1 className: \"UPDATEWorker\" timeOut: 180000 timerStrategy: \"FIXED\"","title":"SPARQL UPDATE Worker"},{"location":"usage/workers/#cli-workers","text":"These workers can be used to benchmark a CLI application.","title":"CLI Workers"},{"location":"usage/workers/#cli-worker","text":"This Worker should be used if the CLI application runs a query once and exits afterwards. Something like $ cli-script.sh query HEADER QUERY RESULT 1 QUERY RESULT 2 ... $ Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIWorker\"","title":"CLI Worker"},{"location":"usage/workers/#cli-input-worker","text":"This Worker should be used if the CLI application runs and the query will be send using the Input. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\"","title":"CLI Input Worker"},{"location":"usage/workers/#multiple-cli-input-worker","text":"This Worker should be used if the CLI application runs and the query will be send using the Input and will quit on errors. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: ERROR ERROR happend, exiting $ To assure a smooth benchmark, the CLI application will be run multiple times instead of once, and if the application quits, the next running process will be used, while in the background the old process will be restarted. Thus as soon as an error happend, the benchmark can continue without a problem. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see above. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"MultipleCLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\"","title":"Multiple CLI Input Worker"},{"location":"usage/workers/#cli-input-file-worker","text":"Same as the Multiple CLI Input Worker . However the query won't be send to the input but written to a file and the file will be send to the input Something like $ cli-script.sh start Your Input: file-containg-the-query.txt HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend directory no Directory in which the file including the query should be saved. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputFileWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" directory: \"/tmp/\"","title":"CLI Input File Worker"},{"location":"usage/workers/#cli-input-prefix-worker","text":"Same as the Multiple CLI Input Worker . However the CLI application might need a pre and suffix. Something like $ cli-script.sh start Your Input: PREFIX QUERY SUFFIX HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend queryPrefix no String to use as a PREFIX before the query. querySuffix no String to use as a SUFFIX after the query. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputPrefixWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" queryPrefix: \"SPARQL\" querySuffix: \";\" Will send the following as Input SPARQL QUERY ;","title":"CLI Input Prefix Worker"},{"location":"usage/workflow/","text":"Workflow Iguana will first parse configuration and afterwards will execute each task for each connection for each dataset. Imagine it like the following: for each dataset D for each connection C for each task T execute pre script hook execute task T(D, C) collect and calculate results write results execute post script hook","title":"Workflow"},{"location":"usage/workflow/#workflow","text":"Iguana will first parse configuration and afterwards will execute each task for each connection for each dataset. Imagine it like the following: for each dataset D for each connection C for each task T execute pre script hook execute task T(D, C) collect and calculate results write results execute post script hook","title":"Workflow"}]}